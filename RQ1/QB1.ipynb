{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/21fl11/.local/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2025-02-24 21:15:31.903862: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-24 21:15:31.921036: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740431731.941913 1284124 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740431731.949675 1284124 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-24 21:15:31.973288: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package punkt to /home/21fl11/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import faiss\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# Initialize NLP tools\n",
    "nltk.download('punkt')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompts\n",
    "system_prompt = \"\"\"\n",
    "You are a knowledgeable and helpful assistant. The user has asked a question on Stack Overflow. \n",
    "Use the provided context to craft an accurate, concise, and highly relevant response. \n",
    "Present your answer in a clear and well-structured paragraph format, avoiding the use of bullet points or lists.\n",
    "DO NOT GENREATE INCOMPLETE CODE AND EXCESSIVE CODE TO DISTRACT PEOPLE!\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "### QUESTION:\n",
    "{question}\n",
    "\n",
    "### CONTEXT:\n",
    "{context}\n",
    "\n",
    "Please provide your best answer below:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline_path = \"complete_testing_data/SAMPLE_combined_unseen_data.csv\"\n",
    "baseline_path = 'Sythetic_old_question.csv'\n",
    "embeddings_path = \"../AnswerEmbedding/all_embeddings.npy\"\n",
    "sentences_path = \"../AnswerEmbedding/all_sentences.npy\"\n",
    "\n",
    "# Load baseline data\n",
    "baseline = pd.read_csv(baseline_path)\n",
    "testingset = baseline.dropna(subset=['Accepted Answer Body'])\n",
    "# testingset = baseline.dropna(subset=['Accepted Answer'])\n",
    "# Load embeddings and sentences\n",
    "sentence_embedding = np.load(embeddings_path, allow_pickle=True)\n",
    "sentence_context = np.load(sentences_path, allow_pickle=True)\n",
    "\n",
    "# Initialize the encoder model and FAISS index\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "encoder_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2').to(device)\n",
    "\n",
    "dim = sentence_embedding.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "index.add(sentence_embedding)\n",
    "\n",
    "# Prepare the list of testing questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "class EndOfAnswerCriteria(StoppingCriteria):\n",
    "    def __init__(self, stop_string: str, tokenizer):\n",
    "        self.stop_string = stop_string\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        decoded = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "        return self.stop_string in decoded\n",
    "\n",
    "def compose_prompt(question, top_results):\n",
    "    sentences_only = [i[1] for i in top_results if len(i) > 0]\n",
    "    context_str = \"\\n\".join(sentences_only)\n",
    "    final_prompt = prompt_template.format(question=question, context=context_str)\n",
    "    return system_prompt, final_prompt\n",
    "\n",
    "def process_rag_with_threshold(threshold, max_results=10):    # Load baseline data\n",
    "    \n",
    "    local_testingset = testingset.copy()\n",
    "    # testing_question = local_testingset['Paraphrased Question'].to_list()\n",
    "    testing_question = local_testingset['Title'].to_list()\n",
    "    # Search and filter results based on the threshold\n",
    "    SEN_LIST = []\n",
    "    for question_title in tqdm(testing_question):\n",
    "        query_embedding = encoder_model.encode(question_title, convert_to_tensor=True).cpu().numpy()\n",
    "        distances, indices = index.search(np.array([query_embedding]), index.ntotal)\n",
    "\n",
    "        filtered_results = [\n",
    "            (idx, sentence_context[idx], dist) \n",
    "            for idx, dist in zip(indices[0], distances[0]) if dist >= threshold\n",
    "        ][:max_results]\n",
    "\n",
    "        entry = {\n",
    "            \"question\": question_title,\n",
    "            \"results\": filtered_results\n",
    "        }\n",
    "        SEN_LIST.append(entry)\n",
    "\n",
    "    # Create prompts\n",
    "    user_msg_list = []\n",
    "    for entry in SEN_LIST:\n",
    "        question = entry[\"question\"]\n",
    "        top_results = entry[\"results\"]\n",
    "        sys_msg, user_msg = compose_prompt(question, top_results)\n",
    "        user_msg_list.append(user_msg)\n",
    "\n",
    "    local_testingset['Step2PROMPT'] = user_msg_list\n",
    "\n",
    "    # Define the Llama model for response generation\n",
    "    model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '<PAD>'})\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    text_generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device_map=\"auto\",\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7,\n",
    "        repetition_penalty=1.1,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "    stop_token = \"END_OF_ANSWER\"\n",
    "    stopping_criteria = StoppingCriteriaList([EndOfAnswerCriteria(stop_token, tokenizer)])\n",
    "\n",
    "    user_prompts = local_testingset[\"Step2PROMPT\"].to_list()\n",
    "    response_list = []\n",
    "\n",
    "    for user_prompt in tqdm(user_prompts):\n",
    "        outputs = text_generator(user_prompt, stopping_criteria=stopping_criteria)\n",
    "        result = outputs[0][\"generated_text\"]\n",
    "        if stop_token in result:\n",
    "            result = result.split(stop_token)[0].strip()\n",
    "        response_list.append(result)\n",
    "\n",
    "    # Clean responses\n",
    "    new_list = []\n",
    "    for response in response_list:\n",
    "        if '[/INST]' in response:\n",
    "            cleaned = response.split('[/INST]', 1)[1].strip()\n",
    "        else:\n",
    "            cleaned = response.strip()\n",
    "        new_list.append(cleaned)\n",
    "\n",
    "    local_testingset['Step2Response'] = new_list\n",
    "    testingset_output = local_testingset.drop(columns=['Generated Response'], errors='ignore')\n",
    "    return testingset_output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [5:49:31<00:00, 54.47s/it]  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed6eaa35d4443d6b7b0fb9e1f6363c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/385 [00:00<?, ?it/s]Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "  3%|▎         | 10/385 [08:48<5:05:41, 48.91s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "100%|██████████| 385/385 [5:53:10<00:00, 55.04s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish  0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "threhsold_list = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "for tl in threhsold_list:\n",
    "    results = process_rag_with_threshold(tl)\n",
    "    # path = \"unseen_testing/RAGv1_threshold_{}.csv\".format(tl)\n",
    "    path = \"sythetic_testing/QB1_threshold_{}.csv\".format(tl)\n",
    "    results.to_csv(path, index=False)\n",
    "    print(\"finish \",tl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
