{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/21fl11/.local/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2025-03-24 03:49:53.267908: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-24 03:49:53.281620: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742788193.298857 2921434 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742788193.304188 2921434 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-24 03:49:53.322005: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package punkt to /home/21fl11/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import faiss\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, StoppingCriteria, StoppingCriteriaList\n",
    "import os\n",
    "import _pickle as cPickle\n",
    "import pickle\n",
    "# Initialize NLP tools\n",
    "nltk.download('punkt')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from huggingface_hub import login\n",
    "login(token=\"\")\n",
    "# Define prompts\n",
    "system_prompt = \"\"\"\n",
    "You are a knowledgeable and helpful assistant. The user has asked a question on Stack Overflow. \n",
    "Use the provided context to craft an accurate, concise, and highly relevant response. \n",
    "Present your answer in a clear and well-structured paragraph format, avoiding the use of bullet points or lists.\n",
    "DO NOT GENREATE INCOMPLETE CODE AND EXCESSIVE CODE TO DISTRACT PEOPLE!\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "### QUESTION:\n",
    "{question}\n",
    "\n",
    "### CONTEXT:\n",
    "{context}\n",
    "\n",
    "Please provide your best answer below:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2921434/156860889.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_embeddings = torch.load(\"../complete_answer_embeddings.pt\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "loaded_embeddings = torch.load(\"../complete_answer_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../answers_context.pkl', 'rb') as f:\n",
    "    answer_sentences_list = pickle.load(f)\n",
    "# type(loaded_answers_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/21fl11/.local/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete.\n"
     ]
    }
   ],
   "source": [
    "ovo_path = \"../CODE_POST_OVERALL-EMBEDDINGS_DATA_V3.pkl\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# 1. Load OVO_data once\n",
    "with open(ovo_path, 'rb') as f:\n",
    "    OVO_data = pickle.load(f)\n",
    "\n",
    "\n",
    "# 2. Extract raw questions, question embeddings, and answer_sentences_list once\n",
    "raw_questions = [item['raw_question'] for item in OVO_data]\n",
    "question_embeddings = [item['question_embedding'] for item in OVO_data]\n",
    "# answer_sentences_list = [item['answer_sentences'] for item in OVO_data]\n",
    "\n",
    "# 3. Build the embeddings matrix once\n",
    "embeddings_matrix = torch.stack(question_embeddings)\n",
    "\n",
    "# 4. Initialize SentenceTransformer once\n",
    "\n",
    "encoder_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2').to(device)\n",
    "\n",
    "print(\"Initialization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define functions\n",
    "class EndOfAnswerCriteria(StoppingCriteria):\n",
    "    def __init__(self, stop_string: str, tokenizer):\n",
    "        self.stop_string = stop_string\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        decoded = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "        return self.stop_string in decoded\n",
    "\n",
    "def compose_prompt(question, relevant_sentences, system_prompt, prompt_template):\n",
    "    sentences_only = [item[\"sentence\"] for item in relevant_sentences]\n",
    "    context_str = \"\\n\".join(sentences_only)\n",
    "    final_prompt = prompt_template.format(question=question, context=context_str)\n",
    "    return system_prompt, final_prompt\n",
    "\n",
    "\n",
    "def find_similar(original_context, top_k, model, embeddings_matrix, question_embeddings, raw_questions, raw_accepted_answers, answer_sentences_list, threshold):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # 1. Encode the original context.\n",
    "    original_embedding = model.encode(original_context, convert_to_tensor=True).to(device)\n",
    "    \n",
    "    # 2. Compute cosine similarities between the original context and the precomputed question embeddings.\n",
    "    #    We assume embeddings_matrix is a torch tensor on CPU; move it to device.\n",
    "    embeddings_matrix = embeddings_matrix.to(device)\n",
    "    cos_scores = util.pytorch_cos_sim(original_embedding, embeddings_matrix).squeeze(0)\n",
    "    \n",
    "    # 3. Get the top-k question indices using torch.topk.\n",
    "    top_k = min(top_k, len(cos_scores))\n",
    "    top_scores, top_indices = torch.topk(cos_scores, k=top_k)\n",
    "    top_indices = top_indices.tolist()  # convert to a list for easier indexing later\n",
    "    \n",
    "    # 4. Collect all answer sentences from the selected top questions.\n",
    "    #    Also record the corresponding question info for each sentence.\n",
    "    sentences = []\n",
    "    sentence_info = []  # to store which question (and its similarity) produced each sentence\n",
    "    for idx in top_indices:\n",
    "        for sent in answer_sentences_list[idx]:\n",
    "            sentences.append(sent)\n",
    "            sentence_info.append({\n",
    "                \"question\": raw_questions[idx],\n",
    "                \"question_similarity\": cos_scores[idx].item()\n",
    "            })\n",
    "    \n",
    "    # 5. If no sentences are found, return empty info.\n",
    "    if not sentences:\n",
    "        return {\n",
    "            \"top_similar_questions\": [raw_questions[idx] for idx in top_indices],\n",
    "            \"sorted_sentences_info\": []\n",
    "        }\n",
    "    \n",
    "    # 6. Batch-encode all collected answer sentences.\n",
    "    answer_embeddings = model.encode(sentences, convert_to_tensor=True).to(device)\n",
    "    \n",
    "    # 7. Compute cosine similarity between the original context and all answer embeddings.\n",
    "    answer_cos_scores = util.pytorch_cos_sim(original_embedding, answer_embeddings).squeeze(0)\n",
    "    \n",
    "    # 8. Filter answers based on the threshold.\n",
    "    valid_mask = answer_cos_scores >= threshold\n",
    "    if valid_mask.sum() == 0:\n",
    "        sorted_sentences_info = []\n",
    "    else:\n",
    "        valid_scores = answer_cos_scores[valid_mask]\n",
    "        valid_indices = torch.nonzero(valid_mask, as_tuple=True)[0]\n",
    "        # Sort valid scores in descending order.\n",
    "        sorted_valid_scores, sorted_order = torch.sort(valid_scores, descending=True)\n",
    "        sorted_valid_indices = valid_indices[sorted_order].tolist()\n",
    "        \n",
    "        sorted_sentences_info = []\n",
    "        for idx in sorted_valid_indices:\n",
    "            sorted_sentences_info.append({\n",
    "                \"sentence\": sentences[idx],\n",
    "                \"similarity\": answer_cos_scores[idx].item(),\n",
    "                \"question\": sentence_info[idx][\"question\"],\n",
    "                \"question_similarity\": sentence_info[idx][\"question_similarity\"]\n",
    "            })\n",
    "    \n",
    "    return {\n",
    "        \"top_similar_questions\": [raw_questions[idx] for idx in top_indices],\n",
    "        \"sorted_sentences_info\": sorted_sentences_info\n",
    "    }\n",
    "\n",
    "def process_rag_with_threshold_v2(threshold, baseline_path= 'Sythetic_old_question.csv', max_results=10):\n",
    "    baseline = pd.read_csv(baseline_path)\n",
    "    testingset = baseline.dropna(subset=['Accepted Answer'])\n",
    "\n",
    "\n",
    "    title_list = testingset['Paraphrased Question'].to_list()\n",
    "    setence_results = []\n",
    "    for question_title in tqdm(title_list):\n",
    "        output = find_similar(question_title, max_results, encoder_model, embeddings_matrix, question_embeddings, raw_questions, None, answer_sentences_list, threshold)\n",
    "        setence_results.append({\n",
    "            \"title\": question_title,\n",
    "            \"relevant_question\": output[\"top_similar_questions\"],\n",
    "            \"relevant_sentence\": output[\"sorted_sentences_info\"]\n",
    "        })\n",
    "\n",
    "    no_relevant_sentences = []\n",
    "    prompt_list = []\n",
    "    for entry in setence_results:\n",
    "        question = entry[\"title\"]\n",
    "        relevant_sentences = entry[\"relevant_sentence\"]\n",
    "\n",
    "        if not relevant_sentences:\n",
    "            no_relevant_sentences.append(0)\n",
    "            sys_msg, user_msg = compose_prompt(question, [], system_prompt, prompt_template)\n",
    "        else:\n",
    "            no_relevant_sentences.append(1)\n",
    "            sys_msg, user_msg = compose_prompt(question, relevant_sentences, system_prompt, prompt_template)\n",
    "        prompt_list.append(user_msg)\n",
    "\n",
    "    testingset['Step2PROMPT_v2'] = prompt_list\n",
    "    testingset['IFUnseen'] = no_relevant_sentences\n",
    "    return testingset\n",
    "\n",
    "def run_llama_model(testingset, model_id=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '<PAD>'})\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    text_generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device_map=\"auto\",\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7,\n",
    "        repetition_penalty=1.1,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "    stop_token = \"END_OF_ANSWER\"\n",
    "    stopping_criteria = StoppingCriteriaList([EndOfAnswerCriteria(stop_token, tokenizer)])\n",
    "\n",
    "    user_prompts = testingset[\"Step2PROMPT_v2\"].to_list()\n",
    "    response_list = []\n",
    "    for user_prompt in tqdm(user_prompts):\n",
    "        outputs = text_generator(user_prompt, stopping_criteria=stopping_criteria)\n",
    "        result = outputs[0][\"generated_text\"]\n",
    "        if stop_token in result:\n",
    "            result = result.split(stop_token)[0].strip()\n",
    "        response_list.append(result)\n",
    "\n",
    "    new_list = []\n",
    "    for response in response_list:\n",
    "        if '[/INST]' in response:\n",
    "            cleaned = response.split('[/INST]', 1)[1].strip()\n",
    "        else:\n",
    "            cleaned = response.strip()\n",
    "        new_list.append(cleaned)\n",
    "\n",
    "    testingset['Step2Response_v2'] = new_list\n",
    "    return testingset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/385 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [16:52<00:00,  2.63s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c0e6e1861745139b84777237e487c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/385 [00:00<?, ?it/s]Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "  3%|▎         | 10/385 [02:13<1:24:02, 13.45s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "100%|██████████| 385/385 [1:27:31<00:00, 13.64s/it]\n",
      "100%|██████████| 385/385 [17:00<00:00,  2.65s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e64d28be58a144919e93adc65b8c3b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [1:28:15<00:00, 13.76s/it]\n",
      "100%|██████████| 385/385 [16:42<00:00,  2.60s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eca57dd5c414bd9a077de87fae78e4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [1:54:26<00:00, 17.83s/it]\n",
      "100%|██████████| 385/385 [15:52<00:00,  2.47s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0561ccf71d342569edba8591a45bb52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [1:29:12<00:00, 13.90s/it]\n",
      "100%|██████████| 385/385 [15:41<00:00,  2.44s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c87c2ef21945a98233a73f1d5d2195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [1:55:41<00:00, 18.03s/it]\n",
      "100%|██████████| 385/385 [15:45<00:00,  2.45s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f61096b28c74631897cc3fc7e34697a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [1:46:33<00:00, 16.61s/it]\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "threhsold_list = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "\n",
    "for tl in threhsold_list:\n",
    "    results = process_rag_with_threshold_v2(tl)\n",
    "    results = run_llama_model(results)\n",
    "    path = \"sythetic_testing/QB4_threshold_{}.csv\".format(tl)\n",
    "    results.to_csv(path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
