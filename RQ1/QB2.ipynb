{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/21fl11/.local/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2025-03-21 20:31:34.118136: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-21 20:31:34.133740: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742589094.151349 1603447 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742589094.156811 1603447 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-21 20:31:34.175237: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package punkt to /home/21fl11/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import faiss\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# Initialize NLP tools\n",
    "nltk.download('punkt')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompts\n",
    "system_prompt = \"\"\"\n",
    "You are a knowledgeable and helpful assistant. The user has asked a question on Stack Overflow. \n",
    "Use the provided context to craft an accurate, concise, and highly relevant response. \n",
    "Present your answer in a clear and well-structured paragraph format, avoiding the use of bullet points or lists.\n",
    "DO NOT GENREATE INCOMPLETE CODE AND EXCESSIVE CODE TO DISTRACT PEOPLE!\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "### QUESTION:\n",
    "{question}\n",
    "\n",
    "### CONTEXT:\n",
    "{context}\n",
    "\n",
    "Please provide your best answer below:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1603447/759465158.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_embeddings = torch.load(\"../complete_answer_embeddings.pt\")\n"
     ]
    }
   ],
   "source": [
    "loaded_embeddings = torch.load(\"../complete_answer_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "ovo_path = \"../CODE_POST_OVERALL-EMBEDDINGS_DATA_V3_complete_answer_embeddings.pkl\"\n",
    "\n",
    "# 1. Load OVO_data once\n",
    "with open(ovo_path, 'rb') as f:\n",
    "    OVO_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_context = [item['text'] for item in OVO_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline_path = \"complete_testing_data/SAMPLE_combined_unseen_data.csv\"\n",
    "baseline_path = 'Sythetic_old_question.csv'\n",
    "# embeddings_path = \"../AnswerEmbedding/all_embeddings.npy\"\n",
    "# sentences_path = \"../AnswerEmbedding/all_sentences.npy\"\n",
    "\n",
    "# Load baseline data\n",
    "baseline = pd.read_csv(baseline_path)\n",
    "testingset = baseline.dropna(subset=['Accepted Answer'])\n",
    "# testingset = baseline.dropna(subset=['Accepted Answer'])\n",
    "# Load embeddings and sentences\n",
    "# sentence_embedding = np.load(embeddings_path, allow_pickle=True)\n",
    "# sentence_context = np.load(sentences_path, allow_pickle=True)\n",
    "\n",
    "# Initialize the encoder model and FAISS index\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "encoder_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2').to(device)\n",
    "loaded_embeddings = loaded_embeddings.cpu().numpy() \n",
    "dim = loaded_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "index.add(loaded_embeddings)\n",
    "\n",
    "# Prepare the list of testing questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "class EndOfAnswerCriteria(StoppingCriteria):\n",
    "    def __init__(self, stop_string: str, tokenizer):\n",
    "        self.stop_string = stop_string\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        decoded = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "        return self.stop_string in decoded\n",
    "\n",
    "def compose_prompt(question, top_results):\n",
    "    sentences_only = [i[1] for i in top_results if len(i) > 0]\n",
    "    context_str = \"\\n\".join(sentences_only)\n",
    "    final_prompt = prompt_template.format(question=question, context=context_str)\n",
    "    return system_prompt, final_prompt\n",
    "\n",
    "def process_rag_with_threshold(threshold, max_results=10):    # Load baseline data\n",
    "    \n",
    "    local_testingset = testingset.copy()\n",
    "    # testing_question = local_testingset['Paraphrased Question'].to_list()\n",
    "    testing_question = local_testingset['Paraphrased Question'].to_list()\n",
    "    # Search and filter results based on the threshold\n",
    "    SEN_LIST = []\n",
    "    for question_title in tqdm(testing_question):\n",
    "        query_embedding = encoder_model.encode(question_title, convert_to_tensor=True).cpu().numpy()\n",
    "        distances, indices = index.search(np.array([query_embedding]), index.ntotal)\n",
    "\n",
    "        filtered_results = [\n",
    "            (idx, answers_context[idx], dist) \n",
    "            for idx, dist in zip(indices[0], distances[0]) if dist >= threshold\n",
    "        ][:max_results]\n",
    "\n",
    "        entry = {\n",
    "            \"question\": question_title,\n",
    "            \"results\": filtered_results\n",
    "        }\n",
    "        SEN_LIST.append(entry)\n",
    "\n",
    "    # Create prompts\n",
    "    user_msg_list = []\n",
    "    for entry in SEN_LIST:\n",
    "        question = entry[\"question\"]\n",
    "        top_results = entry[\"results\"]\n",
    "        sys_msg, user_msg = compose_prompt(question, top_results)\n",
    "        user_msg_list.append(user_msg)\n",
    "\n",
    "    local_testingset['Step2PROMPT'] = user_msg_list\n",
    "\n",
    "    # Define the Llama model for response generation\n",
    "    model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '<PAD>'})\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    text_generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device_map=\"auto\",\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7,\n",
    "        repetition_penalty=1.1,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "    stop_token = \"END_OF_ANSWER\"\n",
    "    stopping_criteria = StoppingCriteriaList([EndOfAnswerCriteria(stop_token, tokenizer)])\n",
    "\n",
    "    user_prompts = local_testingset[\"Step2PROMPT\"].to_list()\n",
    "    response_list = []\n",
    "\n",
    "    for user_prompt in tqdm(user_prompts):\n",
    "        outputs = text_generator(user_prompt, stopping_criteria=stopping_criteria)\n",
    "        result = outputs[0][\"generated_text\"]\n",
    "        if stop_token in result:\n",
    "            result = result.split(stop_token)[0].strip()\n",
    "        response_list.append(result)\n",
    "\n",
    "    # Clean responses\n",
    "    new_list = []\n",
    "    for response in response_list:\n",
    "        if '[/INST]' in response:\n",
    "            cleaned = response.split('[/INST]', 1)[1].strip()\n",
    "        else:\n",
    "            cleaned = response.strip()\n",
    "        new_list.append(cleaned)\n",
    "\n",
    "    local_testingset['Step2Response'] = new_list\n",
    "    testingset_output = local_testingset.drop(columns=['Generated Response'], errors='ignore')\n",
    "    return testingset_output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage:\n",
    "# threhsold_list = [0.45,0.6,0.7]\n",
    "# for tl in threhsold_list:\n",
    "#     # results = process_rag_with_threshold(tl)\n",
    "#    print(\"step2-1-v2-results/RAGv1_threshold_{}.csv\".format(tl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [49:07<00:00,  7.66s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91db9b48757f46438c81fc5f0edf1470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/385 [00:00<?, ?it/s]Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "  3%|▎         | 10/385 [05:01<3:20:48, 32.13s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "100%|██████████| 385/385 [3:14:34<00:00, 30.32s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish  0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [46:58<00:00,  7.32s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900bd9fa36da4fc8a1f6155144c2d6f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [3:03:50<00:00, 28.65s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish  0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [46:42<00:00,  7.28s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c13ea91679ba4b879f572d159d58e8bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [3:10:37<00:00, 29.71s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish  0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [46:43<00:00,  7.28s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a08881819e48ea91fbf3192c843fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [3:06:23<00:00, 29.05s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish  0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [46:17<00:00,  7.21s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7761154967a4f5585ce6dbe8c48180d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [2:59:55<00:00, 28.04s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish  0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [46:08<00:00,  7.19s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "875a208317c34de1b3269223ac301390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [2:49:13<00:00, 26.37s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish  0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [48:05<00:00,  7.50s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165964db1c4640ad906bdf44ea585363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [2:40:01<00:00, 24.94s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish  0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [48:23<00:00,  7.54s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e9df833ebb340c485c01ea924ca250f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [2:43:16<00:00, 25.45s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish  0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [51:24<00:00,  8.01s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a0860b16c1841828a55ca7fd511e337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [2:43:22<00:00, 25.46s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish  0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "threhsold_list = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "for tl in threhsold_list:\n",
    "    results = process_rag_with_threshold(tl)\n",
    "    # path = \"complete_testing_data_output/RAGv1_threshold_{}_segment6.csv\".format(tl)\n",
    "    # path = \"unseen_testing/RAGv1_threshold_{}.csv\".format(tl)\n",
    "    path = \"sythetic_testing/QB2_threshold_{}.csv\".format(tl)\n",
    "    results.to_csv(path, index=False)\n",
    "    print(\"finish \",tl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
